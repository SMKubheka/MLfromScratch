{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import math\n","import matplotlib.cm as cm \n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["(Xtrain, y_train), (Xtest, y_test) = mnist.load_data()\n","\n","Xtrain = np.float32(Xtrain)\n","y_train = np.int32(y_train)\n","Xtest = np.float32(Xtest)\n","y_test = np.int32(y_test)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def init_params(layer_dims):\n","    np.random.seed(3)\n","    params = {}\n","    L = len(layer_dims)\n","    \n","    for l in range(1, L):\n","        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n","        params['b'+str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","    return params"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Z (linear hypothesis) - Z = W*X + b , \n","# W - weight matrix, b- bias vector, X- Input \n","\n","def sigmoid(Z):\n","    A = 1/(1+np.exp(np.dot(-1, Z)))\n","    cache = (Z)\n","\n","    return A, cache"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def forward_prop(X, params):\n","    \n","    A = X # input to first layer i.e. training data\n","    caches = []\n","    L = len(params)//2\n","    for l in range(1, L+1):\n","        A_prev = A\n","        \n","        # Linear Hypothesis\n","        Z = np.dot(params['W'+str(l)], A_prev) + params['b'+str(l)] \n","        \n","        # Storing the linear cache\n","        linear_cache = (A_prev, params['W'+str(l)], params['b'+str(l)]) \n","        \n","        # Applying sigmoid on linear hypothesis\n","        A, activation_cache = sigmoid(Z) \n","        \n","         # storing the both linear and activation cache\n","        cache = (linear_cache, activation_cache)\n","        caches.append(cache)\n","    \n","    return A, caches"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def cost_function(A, Y):\n","    m = Y.shape[1]\n","    \n","    cost = (-1/m)*(np.dot(np.log(A), Y.T) + np.dot(log(1-A), 1-Y.T)) \n","    \n","    return cost"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def one_layer_backward(dA, cache):\n","    linear_cache, activation_cache = cache\n","    \n","    Z = activation_cache\n","    dZ = dA*sigmoid(Z)*(1-sigmoid(Z)) # The derivative of the sigmoid function\n","    \n","    A_prev, W, b = linear_cache\n","    m = A_prev.shape[1]\n","    \n","    dW = (1/m)*np.dot(dZ, A_prev.T)\n","    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def backprop(AL, Y, caches):\n","    grads = {}\n","    L = len(caches)\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape)\n","    \n","    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n","    \n","    current_cache = caches[L-1]\n","    grads['dA'+str(L-1)], grads['dW'+str(L-1)], grads['db'+str(L-1)] = one_layer_backward(dAL, current_cache)\n","    \n","    for l in reversed(range(L-1)):\n","        \n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = one_layer_backward(grads[\"dA\" + str(l+1)], current_cache)\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","        \n","    return grads"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    L = len(parameters) // 2\n","    \n","    for l in range(L):\n","        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] -learning_rate*grads['W'+str(l+1)]\n","        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] -  learning_rate*grads['b'+str(l+1)]\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def train(X, Y, layer_dims, epochs, lr):\n","    params = init_params(layer_dims)\n","    cost_history = []\n","    \n","    for i in range(epochs):\n","        Y_hat, caches = forward_prop(X, params)\n","        cost = cost_function(Y_hat, Y)\n","        cost_history.append(cost)\n","        grads = backprop(Y_hat, Y, caches)\n","        \n","        params = update_parameters(params, grads, lr)\n","        \n","        \n","    return params, cost_history"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"shapes (128,784) and (60000,28,28) not aligned: 784 (dim 1) != 28 (dim 1)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(Xtrain,y_train,[\u001b[39m784\u001b[39;49m,\u001b[39m128\u001b[39;49m,\u001b[39m64\u001b[39;49m,\u001b[39m10\u001b[39;49m],\u001b[39m35\u001b[39;49m,\u001b[39m0.001\u001b[39;49m)\n","\u001b[1;32m/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, layer_dims, epochs, lr)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000038?line=2'>3</a>\u001b[0m cost_history \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000038?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000038?line=5'>6</a>\u001b[0m     Y_hat, caches \u001b[39m=\u001b[39m forward_prop(X, params)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000038?line=6'>7</a>\u001b[0m     cost \u001b[39m=\u001b[39m cost_function(Y_hat, Y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000038?line=7'>8</a>\u001b[0m     cost_history\u001b[39m.\u001b[39mappend(cost)\n","\u001b[1;32m/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb Cell 6'\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(X, params)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000033?line=6'>7</a>\u001b[0m A_prev \u001b[39m=\u001b[39m A\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000033?line=8'>9</a>\u001b[0m \u001b[39m# Linear Hypothesis\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000033?line=9'>10</a>\u001b[0m Z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(params[\u001b[39m'\u001b[39;49m\u001b[39mW\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(l)], A_prev) \u001b[39m+\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(l)] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000033?line=11'>12</a>\u001b[0m \u001b[39m# Storing the linear cache\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smkubheka/Developer/MLfromScratch/Neural_Network/mnist-neural-network-from-scratch-9a5cc4.ipynb#ch0000033?line=12'>13</a>\u001b[0m linear_cache \u001b[39m=\u001b[39m (A_prev, params[\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(l)], params[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(l)]) \n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (128,784) and (60000,28,28) not aligned: 784 (dim 1) != 28 (dim 1)"]}],"source":["train(Xtrain,y_train,[784,128,64,10],35,0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"abfa3a40ca4ae14957eab2c7b2340affb65e642fccfdc8bca44a4575b0473079"},"kernelspec":{"display_name":"Python 3.9.12 ('tf_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
